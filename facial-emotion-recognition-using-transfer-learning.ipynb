{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13215089,"sourceType":"datasetVersion","datasetId":8376054}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zulqarnain11/facial-emotion-recognition-using-transfer-learning?scriptVersionId=267018805\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#Facial Emotion Recognition using Transfer Learning (MobileNetV2)","metadata":{"id":"WP1UQfa2liyl"}},{"cell_type":"markdown","source":"## üè∑Ô∏è Description\n\nThis notebook uses Transfer Learning with MobileNetV2 to recognize facial emotions from images.\nThe model is fine-tuned on a custom dataset and achieves high accuracy by leveraging pretrained ImageNet features.","metadata":{"id":"xMgRpLAPlnTQ"}},{"cell_type":"markdown","source":"## üß© 1: Import Libraries","metadata":{"id":"-BIgQcHhl1-b"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')\n\n","metadata":{"id":"_LjGpBphNYnc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üñºÔ∏è 2: Load Dataset","metadata":{"id":"RePVoHWIl5WL"}},{"cell_type":"code","source":"IMAGE_SIZE = 96\nBATCH_SIZE = 32\nDATASET_DIR = '/kaggle/input/facial-emotion-recognition-dataset/processed_data'\n\ndataset = tf.keras.preprocessing.image_dataset_from_directory(\n    DATASET_DIR,\n    seed=123,\n    shuffle=True,\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE\n)\n\nclass_names = dataset.class_names\nn_classes = len(class_names)\nprint(\"Classes:\", class_names)\n","metadata":{"id":"zDf9WP_MNa-M","outputId":"5d26d672-c891-44d3-ebaa-30290ed6b27a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"Dqa6f4Vpl-NJ"}},{"cell_type":"markdown","source":"## üîÄ 3: Split Dataset into Train, Validation, and Test Sets","metadata":{"id":"KAlrrjuLmOl5"}},{"cell_type":"code","source":"dataset_size = len(list(dataset))\ntrain_size = int(0.8 * dataset_size)\nval_size = int(0.1 * dataset_size)\n\ntrain_ds = dataset.take(train_size)\nremaining = dataset.skip(train_size)\nval_ds = remaining.take(val_size)\ntest_ds = remaining.skip(val_size)\n\nprint(\"Train batches:\", len(train_ds))\nprint(\"Validation batches:\", len(val_ds))\nprint(\"Test batches:\", len(test_ds))\n\n","metadata":{"id":"5x0ndpFSO5Pp","outputId":"e74b5e61-28e7-4088-d1f1-61c686963540"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##‚öôÔ∏è 4: Optimize Dataset Pipeline","metadata":{"id":"KloNC5gkmauD"}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","metadata":{"id":"h7xMQXlBgBR0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##üé® 5: Data Augmentation\n\n","metadata":{"id":"G4loHaeKme_G"}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.15),\n    layers.RandomZoom(0.1),\n    layers.RandomContrast(0.1),\n])\n","metadata":{"id":"--Z84xmNgEYb"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß† 6: Build Transfer Learning Model (MobileNetV2)\n\n","metadata":{"id":"ENKn1ZqfmzN0"}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(\n    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n    include_top=False,\n    weights='imagenet'\n)\nbase_model.trainable = False\n\nmodel = tf.keras.Sequential([\n    data_augmentation,\n    layers.Rescaling(1./255),\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.4),\n    layers.Dense(n_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n","metadata":{"id":"xPkxVc9lgJb0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ 7: Train the Model\n","metadata":{"id":"CUvyYXx4m5rW"}},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n    ModelCheckpoint('best_model.h5', save_best_only=True)\n]\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=callbacks\n)\n","metadata":{"id":"QckjvzGGgMoH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä 8: Evaluate Performance","metadata":{"id":"MkP4EmncnEn4"}},{"cell_type":"code","source":"loss, acc = model.evaluate(test_ds)\nprint(f\"‚úÖ Test Accuracy: {acc*100:.2f}%\")\n\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title(\"Training & Validation Accuracy\")\nplt.show()\n","metadata":{"id":"b_p09Or7gPwM"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà 10: Confusion Matrix\n","metadata":{"id":"di4Bsr4Dnm1j"}},{"cell_type":"code","source":"y_true = []\ny_pred = []\n\nfor images, labels in test_ds:\n    preds = model.predict(images)\n    y_true.extend(labels.numpy())\n    y_pred.extend(np.argmax(preds, axis=1))\n\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\ndisp.plot(xticks_rotation=45, cmap='Blues')\nplt.title(\"Confusion Matrix - Facial Emotion Recognition\")\nplt.show()\n","metadata":{"id":"lupD-ICdgTS0"},"outputs":[],"execution_count":null}]}